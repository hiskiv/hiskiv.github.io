---
layout: post
title: 【论文笔记】ResNet：残差网络
description: >
  何凯明等人提出的Residual Network在2015年的ImageNet挑战赛上大放异彩。只是一个简单的网络构建技巧竟能达到如此优秀的效果。
tags: [论文笔记]
---

## 【论文笔记】ResNet：残差网络

论文原文链接：[Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385)

自AlexNet在ILSVRC上大放异彩之后，CNN-based的方法在图像识别领域便成为了主流。在随后提出的GoogleNet与VGG系列模型中，作者都一定程度上采用了增加网络深度的方式来提高准确率。何凯明等人提出了Residual Network，在ImageNet挑战赛上将错误率由2014年GoogleNet的6.7%降低到了3.57%。



### 从一个问题说起

增加网络的层数（深度）一定可以提高模型的泛化能力吗？

对于早期最普通的全连接网络而言，往往使用Sigmoid函数作为激活函数，由于Sigmoid函数在稍微大一点的位置梯度便趋近为0，因此在一个很深的网络中极有可能导致某个神经元的梯度退化到0，使得反向传播后面的梯度均被限制住，即所谓梯度消失问题。通过使用ReLU激活函数以及加入批归一化层(Batch Normalization)后，梯度消失问题基本能够解决，模型能够保证收敛。

但在继续增大网络层数时，人们发现，错误率不降反升。如下图，我们发现不论是在训练集还是测试集上，在增加层数后误差均升高。由此可知该现象不是因过拟合产生的（否则训练集误差应更低）。在论文中将该现象成为`degradation`。

> 疑问：产生该现象的理论依据是什么？

<img src="http://tva1.sinaimg.cn/large/008qPTh8ly1h3w2zvwkcpj30nr0cbn4c.jpg" alt="image.png" style="zoom:45%;" />

Degradation现象的存在说明并非所有问题都能够简单地通过普通网络模型来进行拟合。但我们至少能够人为地构造一个网络，使得较深模型得到的结果并不比层数较浅的模型来得差。构造方法颇为直接：我们可以在浅层模型的基础上，在后面加上更深的层，并调整新加入层的权重，使其成为单位映射。这个构造便是Residual概念一个最核心的intuition。



### Residual Learning

#### 残差映射

在上面，我们构造了一个单位映射，然而大多数模型中神经元采用的都是非线性层（如Sigmoid，ReLU等）。利用非线性层去拟合一个线性（单位）映射往往效果并不佳。

本论文创造性地提出了Residual Learning：设目标拟合函数为$\mathcal{H}(x)$，我们学习的非线性层参数用于拟合函数$\mathcal{F}(x)=\mathcal{H}(x)-x$。作者发现，最优化该残差函数比直接优化原函数更容易（深层次原因可能得等下学期学了优化理论之后才能理解）。在极端情况下，$\mathcal{H}(x)=x$，此时直接将参数函数$\mathcal{F}(x)$拟合至0比直接拟合原函数到恒等映射的难度显然更简单一些。

这里突然想到，如果目标函数$\mathcal{H}(x)=0$，或$\mathcal{H}(x)=2x$，那么残差函数可能并没有更优。但作者也在文中提到，当目标函数比起零映射更接近恒等映射时，这个优化能够提供一个好的预处理效果。

#### 残差块

作者提出的一个基本残差块的结构如下图所示：

<img src="http://tva1.sinaimg.cn/large/008qPTh8ly1h3w3vaw6q7j30g108fac8.jpg" alt="image.png" style="zoom:50%;" />

即$y = \sigma(\mathcal{F}(x, \{W_i\})+x)$，其中$\mathcal{F}(x,\{W_i\})=W_2 \sigma(W_1 x)$，$\sigma$为非线性激活函数（在该式中省略了bias）。注意，在上式中我们需要保证该$x$的维数与经$\mathcal{F}$作用后的维数相等。对于维数不等的情况，主要有两种解决方法：

1. 对加上的$x$进行投影操作
2. 对加上的$x$，在多出的分量上补零

在上图所示的残差块中，残差函数包括两个全连接层。事实上两个以上全连接层均可以尝试，但是只有一个全连接层时该残差块就退化为$y=\sigma(W_1 x + x)$的形式，与原本普通层并无差异。此外，残差块还可以应用到卷积层上。

#### 网络结构

作者提出的ResNet网络结构如下图：

<img src="http://tva1.sinaimg.cn/large/008qPTh8ly1h3w4lorgqcj30nq0tpwpm.jpg" alt="image.png" style="zoom:45%;" />

首先，基本卷积层与池化层的构建思路来自于VGG模型系列，即多采用$3\times3$大小的卷积核，同时通过调整filter数目来尽可能地维持每一层的complexity。此外，将每两个卷积层构建一个残差块，而对于池化卷积层残差块（虚线）则使用上面所述的方式进行维数调整。



### 测试

作者分别对18层与34层的plain net及ResNet进行了对比，发现增大层数后plain net的误差不降反升，可能是因为网络太深，当前的SGD算法无法快速使其收敛（收敛速度指数级的慢）；而对于ResNet，增大层数后准确率确实有明显的提高。

<img src="http://tva1.sinaimg.cn/large/008qPTh8ly1h3wzbx1xxfj316i0dkdon.jpg" alt="image.png" style="zoom:50%;" />



#### Shortcuts 的影响

记得我们曾在上面提到过，对于残差块前后维数不等的情况（比如在池化或降采样阶段），我们可以选用不同的策略进行构建。作者对三种策略进行了测试：

A. 零扩充

B. 对增加的维数通过投影映射得到，其余原有的维数保持恒等映射

C. 对整个shortcut做投影映射

测试结果如下图第二栏所示。可见三种方式得到的误差均比plain net更低，但是三者间相差不大。

<img src="http://tva1.sinaimg.cn/large/008qPTh8ly1h3wzgqaracj30n60h3wni.jpg" alt="image.png" style="zoom:50%;" />

最后作者还提到，ResNet具有极强的泛化能力，除了ImageNet上的结果，其在CIFAR-10、COCO的目标检测、图像分割等领域上都取得了极好的成果。



### 感想

读完ResNet，不禁感叹intuition的重要性。整篇论文其实最核心的东西就在于将拟合原函数转化成了拟合残差函数，而这一灵感来源于对于degradation问题一个平凡的构造。对此我还是存在一定疑问，难道在实际应用中大多数问题比起零映射都更接近恒等映射吗？为什么ResNet能起到这么显著的提升呢？

> 读完后续需要精通：Batch Normalization